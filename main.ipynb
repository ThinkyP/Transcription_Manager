{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import assemblyai as aai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Import and configure AssemblyAI\n",
    "import assemblyai as aai\n",
    "\n",
    "# Set your API key\n",
    "aai.settings.api_key = \"c34c3778e3a449dca16e926e0589e2a1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Transcription Complete\n",
      "\n",
      "Well, Casey, as you know, I am writing a book. Yes. And congratulations. I can't wait to read it. Yeah, I can't wait to write it. So the book is called the AGI Chronicles. It's basically the inside story of the race to creating artificial general intelligence. Now, here's a question. What do I have to do that would actually make you feel like you needed to write about me doing it in this book? Do you know what I mean? Like, what sort of effect would I need to have on the development of AI for yo...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Upload audio and transcribe it\n",
    "transcriber = aai.Transcriber()\n",
    "\n",
    "# Transcribe the audio file (sync method)\n",
    "transcript = transcriber.transcribe(\"podcast.mpga\", model=\"nova-2\")\n",
    "\n",
    "# Optional: Print transcript to confirm\n",
    "print(\"âœ… Transcription Complete\\n\")\n",
    "print(transcript.text[:500] + \"...\")  # Show first 500 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Transcription saved to 'transcription_output.txt'\n"
     ]
    }
   ],
   "source": [
    "# Write it to a .txt file\n",
    "with open(\"transcription_output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(transcript.text)\n",
    "\n",
    "print(\"âœ… Transcription saved to 'transcription_output.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_client',\n",
       " '_executor',\n",
       " '_impl',\n",
       " 'audio_duration',\n",
       " 'audio_url',\n",
       " 'auto_highlights',\n",
       " 'chapters',\n",
       " 'confidence',\n",
       " 'config',\n",
       " 'content_safety',\n",
       " 'delete_by_id',\n",
       " 'delete_by_id_async',\n",
       " 'entities',\n",
       " 'error',\n",
       " 'export_subtitles_srt',\n",
       " 'export_subtitles_vtt',\n",
       " 'from_response',\n",
       " 'get_by_id',\n",
       " 'get_by_id_async',\n",
       " 'get_paragraphs',\n",
       " 'get_redacted_audio_url',\n",
       " 'get_sentences',\n",
       " 'iab_categories',\n",
       " 'id',\n",
       " 'json_response',\n",
       " 'lemur',\n",
       " 'save_redacted_audio',\n",
       " 'sentiment_analysis',\n",
       " 'speech_model',\n",
       " 'status',\n",
       " 'summary',\n",
       " 'text',\n",
       " 'utterances',\n",
       " 'wait_for_completion',\n",
       " 'wait_for_completion_async',\n",
       " 'webhook_auth',\n",
       " 'webhook_status_code',\n",
       " 'word_search',\n",
       " 'words']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LemurError",
     "evalue": "failed to call Lemur task: Your account does not have access to LeMUR. Please upgrade or contact us at support@assemblyai.com for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLemurError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarize the key points discussed in the podcast.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Send the prompt to the Lemur model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m transcript\u001b[38;5;241m.\u001b[39mlemur\u001b[38;5;241m.\u001b[39mtask(\n\u001b[1;32m      6\u001b[0m     prompt, final_model\u001b[38;5;241m=\u001b[39maai\u001b[38;5;241m.\u001b[39mLemurModel\u001b[38;5;241m.\u001b[39mclaude3_7_sonnet_20250219\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Display result\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ§  Lemur Task Result:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/assemblyai/lemur.py:467\u001b[0m, in \u001b[0;36mLemur.task\u001b[0;34m(self, prompt, context, final_model, max_output_size, timeout, temperature, input_text)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtask\u001b[39m(\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    442\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m     input_text: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m types\u001b[38;5;241m.\u001b[39mLemurTaskResponse:\n\u001b[1;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m    Task feature allows you to submit a custom prompt to the model.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    Returns: A response to a question or task submitted via custom prompt (with source transcripts or other sources taken into the context)\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl\u001b[38;5;241m.\u001b[39mtask(\n\u001b[1;32m    468\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    469\u001b[0m         context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    470\u001b[0m         final_model\u001b[38;5;241m=\u001b[39mfinal_model,\n\u001b[1;32m    471\u001b[0m         max_output_size\u001b[38;5;241m=\u001b[39mmax_output_size,\n\u001b[1;32m    472\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    473\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    474\u001b[0m         input_text\u001b[38;5;241m=\u001b[39minput_text,\n\u001b[1;32m    475\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/assemblyai/lemur.py:113\u001b[0m, in \u001b[0;36m_LemurImpl.task\u001b[0;34m(self, prompt, context, final_model, max_output_size, timeout, temperature, input_text)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtask\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    105\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m     input_text: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    112\u001b[0m ):\n\u001b[0;32m--> 113\u001b[0m     response \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mlemur_task(\n\u001b[1;32m    114\u001b[0m         client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mhttp_client,\n\u001b[1;32m    115\u001b[0m         request\u001b[38;5;241m=\u001b[39mtypes\u001b[38;5;241m.\u001b[39mLemurTaskRequest(\n\u001b[1;32m    116\u001b[0m             sources\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sources,\n\u001b[1;32m    117\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    118\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    119\u001b[0m             final_model\u001b[38;5;241m=\u001b[39mfinal_model,\n\u001b[1;32m    120\u001b[0m             max_output_size\u001b[38;5;241m=\u001b[39mmax_output_size,\n\u001b[1;32m    121\u001b[0m             temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    122\u001b[0m             input_text\u001b[38;5;241m=\u001b[39minput_text,\n\u001b[1;32m    123\u001b[0m         ),\n\u001b[1;32m    124\u001b[0m         http_timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/assemblyai/api.py:366\u001b[0m, in \u001b[0;36mlemur_task\u001b[0;34m(client, request, http_timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENDPOINT_LEMUR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/task\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    359\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdict(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mhttp_timeout,\n\u001b[1;32m    363\u001b[0m )\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mOK:\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m types\u001b[38;5;241m.\u001b[39mLemurError(\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed to call Lemur task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_get_error_message(response)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    368\u001b[0m         response\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m types\u001b[38;5;241m.\u001b[39mLemurTaskResponse\u001b[38;5;241m.\u001b[39mparse_obj(response\u001b[38;5;241m.\u001b[39mjson())\n",
      "\u001b[0;31mLemurError\u001b[0m: failed to call Lemur task: Your account does not have access to LeMUR. Please upgrade or contact us at support@assemblyai.com for more information."
     ]
    }
   ],
   "source": [
    "# Step 4: Use Lemur for a task (e.g., summarization)\n",
    "prompt = \"Summarize the key points discussed in the podcast.\"\n",
    "\n",
    "# Send the prompt to the Lemur model\n",
    "result = transcript.lemur.task(\n",
    "    prompt, final_model=aai.LemurModel.claude3_7_sonnet_20250219\n",
    "\n",
    ")\n",
    "\n",
    "# Display result\n",
    "print(\"ðŸ§  Lemur Task Result:\\n\")\n",
    "print(result.response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
